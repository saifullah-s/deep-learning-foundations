# Deep Learning - Foundational Projects

## Overview  
Completed several deep learning projects focusing on core concepts related to neural networks. These projects provided a deep dive into the foundations of neural networks, optimization, and sequence modeling, allowing me to "look under the hood". They involved implementing and designing deep learning models from scratch, that is without the help of frameworks such as TensorFlow/Pytorch. This forced me to thoroughly master and understand the underlying mechanics of neural networks, including standard networks, convolutional neural networks and sequence models (RNN, GRU, LSTM). 

**Note**: The various notebooks and code are being revised and prepared for uploading to my portfolio.

## Key Accomplishments and Projects  

### 1. Neural Networks (From Scratch)  
- Built a fully connected deep neural network from scratch using just Python and NumPy.
- Implemented (from scratch without frameworks like TensorFlow/Pytorch) all the core components including:
  - Forward propagation and backpropagation.
  - Activation functions (ReLU, sigmoid, tanh).
  - Optimization algorithms (Gradient Descent, Momentum, Adam, Minibatch GD, Stocastic GD) and understood how different optimization techniques influence convergence and training efficiency.
  - Weight Initializations (random and He Initializations).
  - Regularization to prevent overfitting (L2 and dropout).
  - Loss/Cost function calculation.
  - Vectorization for efficiency.
  - Hyperparameter Tuning (learning rate, regularization rate and others).
- Explored how the depth and structure of a network impact learning and performance.

### 2. Convolutional Neural Networks (CNNs) (From Scratch)  
- Implemented from scratch (without using TensorFlow/Pytorch) a CNN including:
  - Forward propagation and backpropagation.
  - Appropriate padding.
  - Convolutional layers.
  - Pooling (max and average) layers.
  - Fully connected layers.
  - Activation functions.
- Similarly, the ResNet-50 architecture was implemented from scratch, including the Residual blocks.
- Learned how CNNs can be used for applications like object detection and image recognition.

### 3. Sequence Models (RNN, GRU, LSTM)  
- Implemented from scratch a Recurrent Neural Network (RNN) to process sequential data and understand time-series dependencies.
- Built models with GRU and LSTM architectures to address the vanishing gradient problem in longer sequences.

### 4. Data Preprocessing
- Preprocessed data for deep learning pipelines, including normalization and augmentation.

## Tools and Libraries Used  
- Programming Language: Python  
- Deep Learning Frameworks: TensorFlow, Keras  
- Libraries: NumPy, Pandas, Matplotlib  
- Environment: Jupyter Notebook

## Impact and Takeaways  
These foundational projects strengthened my understanding of deep learning and its applications to real-world problems such as image recognition, time-series analysis, and natural language processing. By implementing neural networks and exploring optimization techniques, I gained the skills needed to design and train effective models tailored to various domains.
